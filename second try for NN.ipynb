{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "e62568a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import talib\n",
    "from sklearn.model_selection import train_test_split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "7e1c498b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "25491f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('C://Greece_EQ_Euro_wDS_AIR_CT19.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "7a94671b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Loss</th>\n",
       "      <th>Magnitude</th>\n",
       "      <th>FocalDepth</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.59</td>\n",
       "      <td>21.6</td>\n",
       "      <td>56.854</td>\n",
       "      <td>28.363</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.22</td>\n",
       "      <td>13.7</td>\n",
       "      <td>-29.652</td>\n",
       "      <td>49.738</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.02</td>\n",
       "      <td>4.6</td>\n",
       "      <td>23.078</td>\n",
       "      <td>34.703</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.60</td>\n",
       "      <td>24.3</td>\n",
       "      <td>-10.544</td>\n",
       "      <td>30.328</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.50</td>\n",
       "      <td>2.7</td>\n",
       "      <td>19.881</td>\n",
       "      <td>37.587</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>817246</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.20</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-44.213</td>\n",
       "      <td>27.002</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>817247</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.16</td>\n",
       "      <td>13.3</td>\n",
       "      <td>10.110</td>\n",
       "      <td>45.196</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>817248</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.15</td>\n",
       "      <td>60.3</td>\n",
       "      <td>48.142</td>\n",
       "      <td>33.263</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>817249</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.40</td>\n",
       "      <td>114.9</td>\n",
       "      <td>24.971</td>\n",
       "      <td>36.428</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>817250</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.29</td>\n",
       "      <td>11.9</td>\n",
       "      <td>28.491</td>\n",
       "      <td>38.403</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>817251 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Loss  Magnitude  FocalDepth  Longitude  Latitude    Rate\n",
       "0        0.0       5.59        21.6     56.854    28.363  0.0001\n",
       "1        0.0       5.22        13.7    -29.652    49.738  0.0001\n",
       "2        0.0       5.02         4.6     23.078    34.703  0.0001\n",
       "3        0.0       5.60        24.3    -10.544    30.328  0.0001\n",
       "4        0.0       5.50         2.7     19.881    37.587  0.0001\n",
       "...      ...        ...         ...        ...       ...     ...\n",
       "817246   0.0       5.20         5.0    -44.213    27.002  0.0001\n",
       "817247   0.0       5.16        13.3     10.110    45.196  0.0001\n",
       "817248   0.0       5.15        60.3     48.142    33.263  0.0001\n",
       "817249   0.0       5.40       114.9     24.971    36.428  0.0001\n",
       "817250   0.0       5.29        11.9     28.491    38.403  0.0001\n",
       "\n",
       "[817251 rows x 6 columns]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "8e71becf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Loss</th>\n",
       "      <th>Magnitude</th>\n",
       "      <th>FocalDepth</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Latitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.59</td>\n",
       "      <td>21.6</td>\n",
       "      <td>56.854</td>\n",
       "      <td>28.363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.22</td>\n",
       "      <td>13.7</td>\n",
       "      <td>-29.652</td>\n",
       "      <td>49.738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.02</td>\n",
       "      <td>4.6</td>\n",
       "      <td>23.078</td>\n",
       "      <td>34.703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.60</td>\n",
       "      <td>24.3</td>\n",
       "      <td>-10.544</td>\n",
       "      <td>30.328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.50</td>\n",
       "      <td>2.7</td>\n",
       "      <td>19.881</td>\n",
       "      <td>37.587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>817246</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.20</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-44.213</td>\n",
       "      <td>27.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>817247</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.16</td>\n",
       "      <td>13.3</td>\n",
       "      <td>10.110</td>\n",
       "      <td>45.196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>817248</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.15</td>\n",
       "      <td>60.3</td>\n",
       "      <td>48.142</td>\n",
       "      <td>33.263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>817249</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.40</td>\n",
       "      <td>114.9</td>\n",
       "      <td>24.971</td>\n",
       "      <td>36.428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>817250</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.29</td>\n",
       "      <td>11.9</td>\n",
       "      <td>28.491</td>\n",
       "      <td>38.403</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>817251 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Loss  Magnitude  FocalDepth  Longitude  Latitude\n",
       "0        0.0       5.59        21.6     56.854    28.363\n",
       "1        0.0       5.22        13.7    -29.652    49.738\n",
       "2        0.0       5.02         4.6     23.078    34.703\n",
       "3        0.0       5.60        24.3    -10.544    30.328\n",
       "4        0.0       5.50         2.7     19.881    37.587\n",
       "...      ...        ...         ...        ...       ...\n",
       "817246   0.0       5.20         5.0    -44.213    27.002\n",
       "817247   0.0       5.16        13.3     10.110    45.196\n",
       "817248   0.0       5.15        60.3     48.142    33.263\n",
       "817249   0.0       5.40       114.9     24.971    36.428\n",
       "817250   0.0       5.29        11.9     28.491    38.403\n",
       "\n",
       "[817251 rows x 5 columns]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.drop('Rate',axis=1, inplace= True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "5092b430",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Loss</th>\n",
       "      <th>Magnitude</th>\n",
       "      <th>FocalDepth</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Latitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.037721</td>\n",
       "      <td>0.573079</td>\n",
       "      <td>-0.011757</td>\n",
       "      <td>1.223504</td>\n",
       "      <td>-1.041748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.037721</td>\n",
       "      <td>-0.377694</td>\n",
       "      <td>-0.298926</td>\n",
       "      <td>-1.565259</td>\n",
       "      <td>0.787972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.037721</td>\n",
       "      <td>-0.891626</td>\n",
       "      <td>-0.629715</td>\n",
       "      <td>0.134640</td>\n",
       "      <td>-0.499038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.037721</td>\n",
       "      <td>0.598776</td>\n",
       "      <td>0.086390</td>\n",
       "      <td>-0.949259</td>\n",
       "      <td>-0.873542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.037721</td>\n",
       "      <td>0.341810</td>\n",
       "      <td>-0.698781</td>\n",
       "      <td>0.031576</td>\n",
       "      <td>-0.252165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>817246</th>\n",
       "      <td>-0.037721</td>\n",
       "      <td>-0.429087</td>\n",
       "      <td>-0.615175</td>\n",
       "      <td>-2.034674</td>\n",
       "      <td>-1.158251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>817247</th>\n",
       "      <td>-0.037721</td>\n",
       "      <td>-0.531874</td>\n",
       "      <td>-0.313466</td>\n",
       "      <td>-0.283420</td>\n",
       "      <td>0.399172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>817248</th>\n",
       "      <td>-0.037721</td>\n",
       "      <td>-0.557570</td>\n",
       "      <td>1.395009</td>\n",
       "      <td>0.942648</td>\n",
       "      <td>-0.622303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>817249</th>\n",
       "      <td>-0.037721</td>\n",
       "      <td>0.084844</td>\n",
       "      <td>3.379747</td>\n",
       "      <td>0.195666</td>\n",
       "      <td>-0.351376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>817250</th>\n",
       "      <td>-0.037721</td>\n",
       "      <td>-0.197818</td>\n",
       "      <td>-0.364357</td>\n",
       "      <td>0.309143</td>\n",
       "      <td>-0.182315</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>817251 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Loss  Magnitude  FocalDepth  Longitude  Latitude\n",
       "0      -0.037721   0.573079   -0.011757   1.223504 -1.041748\n",
       "1      -0.037721  -0.377694   -0.298926  -1.565259  0.787972\n",
       "2      -0.037721  -0.891626   -0.629715   0.134640 -0.499038\n",
       "3      -0.037721   0.598776    0.086390  -0.949259 -0.873542\n",
       "4      -0.037721   0.341810   -0.698781   0.031576 -0.252165\n",
       "...          ...        ...         ...        ...       ...\n",
       "817246 -0.037721  -0.429087   -0.615175  -2.034674 -1.158251\n",
       "817247 -0.037721  -0.531874   -0.313466  -0.283420  0.399172\n",
       "817248 -0.037721  -0.557570    1.395009   0.942648 -0.622303\n",
       "817249 -0.037721   0.084844    3.379747   0.195666 -0.351376\n",
       "817250 -0.037721  -0.197818   -0.364357   0.309143 -0.182315\n",
       "\n",
       "[817251 rows x 5 columns]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import scale\n",
    "scale_data= scale(dataset)\n",
    "df_dataset= pd.DataFrame(scale_data, index= dataset.index, columns= dataset.columns)\n",
    "df_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "16731bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "5befd487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2658627. 2485027.       0. ...       0.       0.       0.]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d809d550",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "ede8513d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Dense, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "87d0f35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "b678d953",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(2,input_dim=4, activation = 'relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "b1d53f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "b520156a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'adam', loss = 'mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "1209d638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_10 (Dense)            (None, 2)                 10        \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 1)                 3         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 13\n",
      "Trainable params: 13\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "fb8c1a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "19155/19155 [==============================] - 40s 2ms/step - loss: 123304954835238912.0000 - val_loss: 255073744022667264.0000\n",
      "Epoch 2/1000\n",
      "19155/19155 [==============================] - 40s 2ms/step - loss: 123304825986220032.0000 - val_loss: 255073744022667264.0000\n",
      "Epoch 3/1000\n",
      "19155/19155 [==============================] - 39s 2ms/step - loss: 123305100864126976.0000 - val_loss: 255073709662928896.0000\n",
      "Epoch 4/1000\n",
      "19155/19155 [==============================] - 38s 2ms/step - loss: 123304937655369728.0000 - val_loss: 255073692483059712.0000\n",
      "Epoch 5/1000\n",
      "19155/19155 [==============================] - 38s 2ms/step - loss: 123304619827789824.0000 - val_loss: 255073692483059712.0000\n",
      "Epoch 6/1000\n",
      "19155/19155 [==============================] - 40s 2ms/step - loss: 123304267640471552.0000 - val_loss: 255073640943452160.0000\n",
      "Epoch 7/1000\n",
      "19155/19155 [==============================] - 40s 2ms/step - loss: 123304456619032576.0000 - val_loss: 255073623763582976.0000\n",
      "Epoch 8/1000\n",
      "19155/19155 [==============================] - 40s 2ms/step - loss: 123304705727135744.0000 - val_loss: 255073606583713792.0000\n",
      "Epoch 9/1000\n",
      "19155/19155 [==============================] - 40s 2ms/step - loss: 123304722907004928.0000 - val_loss: 255073555044106240.0000\n",
      "Epoch 10/1000\n",
      "19155/19155 [==============================] - 41s 2ms/step - loss: 123304825986220032.0000 - val_loss: 255073520684367872.0000\n",
      "Epoch 11/1000\n",
      "19155/19155 [==============================] - 41s 2ms/step - loss: 123304422259294208.0000 - val_loss: 255073451964891136.0000\n",
      "Epoch 12/1000\n",
      "19155/19155 [==============================] - 43s 2ms/step - loss: 123304353539817472.0000 - val_loss: 255073400425283584.0000\n",
      "Epoch 13/1000\n",
      "19155/19155 [==============================] - 43s 2ms/step - loss: 123304009942433792.0000 - val_loss: 255073366065545216.0000\n",
      "Epoch 14/1000\n",
      "19155/19155 [==============================] - 41s 2ms/step - loss: 123304439439163392.0000 - val_loss: 255073280166199296.0000\n",
      "Epoch 15/1000\n",
      "19155/19155 [==============================] - 45s 2ms/step - loss: 123304671367397376.0000 - val_loss: 255073211446722560.0000\n",
      "Epoch 16/1000\n",
      "19155/19155 [==============================] - 43s 2ms/step - loss: 123304911885565952.0000 - val_loss: 255073177086984192.0000\n",
      "Epoch 17/1000\n",
      "19155/19155 [==============================] - 42s 2ms/step - loss: 123304164561256448.0000 - val_loss: 255073142727245824.0000\n",
      "Epoch 18/1000\n",
      "19155/19155 [==============================] - 43s 2ms/step - loss: 123304250460602368.0000 - val_loss: 255073091187638272.0000\n",
      "Epoch 19/1000\n",
      "19155/19155 [==============================] - 41s 2ms/step - loss: 123303881093414912.0000 - val_loss: 255073022468161536.0000\n",
      "Epoch 20/1000\n",
      "19155/19155 [==============================] - 34s 2ms/step - loss: 123304422259294208.0000 - val_loss: 255072902209077248.0000\n",
      "Epoch 21/1000\n",
      "19155/19155 [==============================] - 32s 2ms/step - loss: 123303898273284096.0000 - val_loss: 255072781949992960.0000\n",
      "Epoch 22/1000\n",
      "19155/19155 [==============================] - 33s 2ms/step - loss: 123304379309621248.0000 - val_loss: 255072747590254592.0000\n",
      "Epoch 23/1000\n",
      "19155/19155 [==============================] - 32s 2ms/step - loss: 123303898273284096.0000 - val_loss: 255072627331170304.0000\n",
      "Epoch 24/1000\n",
      "19155/19155 [==============================] - 33s 2ms/step - loss: 123303614805442560.0000 - val_loss: 255072592971431936.0000\n",
      "Epoch 25/1000\n",
      "19155/19155 [==============================] - 33s 2ms/step - loss: 123304413669359616.0000 - val_loss: 255072438352609280.0000\n",
      "Epoch 26/1000\n",
      "19155/19155 [==============================] - 46s 2ms/step - loss: 123303941222957056.0000 - val_loss: 255072232194179072.0000\n",
      "Epoch 27/1000\n",
      "19155/19155 [==============================] - 53s 3ms/step - loss: 123303202488582144.0000 - val_loss: 255072111935094784.0000\n",
      "Epoch 28/1000\n",
      "19155/19155 [==============================] - 53s 3ms/step - loss: 123304044302172160.0000 - val_loss: 255071991676010496.0000\n",
      "Epoch 29/1000\n",
      "19155/19155 [==============================] - 53s 3ms/step - loss: 123303013510021120.0000 - val_loss: 255071819877318656.0000\n",
      "Epoch 30/1000\n",
      "19155/19155 [==============================] - 48s 2ms/step - loss: 123303150948974592.0000 - val_loss: 255071682438365184.0000\n",
      "Epoch 31/1000\n",
      "19155/19155 [==============================] - 45s 2ms/step - loss: 123302558243487744.0000 - val_loss: 255071579359150080.0000\n",
      "Epoch 32/1000\n",
      "19155/19155 [==============================] - 51s 3ms/step - loss: 123302532473683968.0000 - val_loss: 255071424740327424.0000\n",
      "Epoch 33/1000\n",
      "19155/19155 [==============================] - 39s 2ms/step - loss: 123303047869759488.0000 - val_loss: 255071287301373952.0000\n",
      "Epoch 34/1000\n",
      "19155/19155 [==============================] - 37s 2ms/step - loss: 123302781581787136.0000 - val_loss: 255071132682551296.0000\n",
      "Epoch 35/1000\n",
      "19155/19155 [==============================] - 37s 2ms/step - loss: 123302704272375808.0000 - val_loss: 255070978063728640.0000\n",
      "Epoch 36/1000\n",
      "19155/19155 [==============================] - 38s 2ms/step - loss: 123302532473683968.0000 - val_loss: 255070823444905984.0000\n",
      "Epoch 37/1000\n",
      "19155/19155 [==============================] - 42s 2ms/step - loss: 123302687092506624.0000 - val_loss: 255070651646214144.0000\n",
      "Epoch 38/1000\n",
      "19155/19155 [==============================] - 41s 2ms/step - loss: 123302180286365696.0000 - val_loss: 255070548566999040.0000\n",
      "Epoch 39/1000\n",
      "19155/19155 [==============================] - 38s 2ms/step - loss: 123302704272375808.0000 - val_loss: 255070325228699648.0000\n",
      "Epoch 40/1000\n",
      "19155/19155 [==============================] - 38s 2ms/step - loss: 123301879638654976.0000 - val_loss: 255070119070269440.0000\n",
      "Epoch 41/1000\n",
      "19155/19155 [==============================] - 39s 2ms/step - loss: 123301999897739264.0000 - val_loss: 255069895731970048.0000\n",
      "Epoch 42/1000\n",
      "19155/19155 [==============================] - 39s 2ms/step - loss: 123301415782187008.0000 - val_loss: 255069655213801472.0000\n",
      "Epoch 43/1000\n",
      "19155/19155 [==============================] - 37s 2ms/step - loss: 123301192443887616.0000 - val_loss: 255069483415109632.0000\n",
      "Epoch 44/1000\n",
      "19155/19155 [==============================] - 37s 2ms/step - loss: 123301089364672512.0000 - val_loss: 255069260076810240.0000\n",
      "Epoch 45/1000\n",
      "19155/19155 [==============================] - 37s 2ms/step - loss: 123300969105588224.0000 - val_loss: 255069036738510848.0000\n",
      "Epoch 46/1000\n",
      "19155/19155 [==============================] - 37s 2ms/step - loss: 123301828099047424.0000 - val_loss: 255068864939819008.0000\n",
      "Epoch 47/1000\n",
      "19155/19155 [==============================] - 38s 2ms/step - loss: 123301252573429760.0000 - val_loss: 255068641601519616.0000\n",
      "Epoch 48/1000\n",
      "19155/19155 [==============================] - 39s 2ms/step - loss: 123300823076700160.0000 - val_loss: 255068469802827776.0000\n",
      "Epoch 49/1000\n",
      "19155/19155 [==============================] - 41s 2ms/step - loss: 123300608328335360.0000 - val_loss: 255068298004135936.0000\n",
      "Epoch 50/1000\n",
      "19155/19155 [==============================] - 43s 2ms/step - loss: 123300960515653632.0000 - val_loss: 255068109025574912.0000\n",
      "Epoch 51/1000\n",
      "19155/19155 [==============================] - 41s 2ms/step - loss: 123300299090690048.0000 - val_loss: 255067937226883072.0000\n",
      "Epoch 52/1000\n",
      "19155/19155 [==============================] - 28s 1ms/step - loss: 123299706385203200.0000 - val_loss: 255067696708714496.0000\n",
      "Epoch 53/1000\n",
      "19155/19155 [==============================] - 28s 1ms/step - loss: 123300196011474944.0000 - val_loss: 255067456190545920.0000\n",
      "Epoch 54/1000\n",
      "19155/19155 [==============================] - 28s 1ms/step - loss: 123300118702063616.0000 - val_loss: 255067250032115712.0000\n",
      "Epoch 55/1000\n",
      "19155/19155 [==============================] - 28s 1ms/step - loss: 123299036370305024.0000 - val_loss: 255067061053554688.0000\n",
      "Epoch 56/1000\n",
      "19155/19155 [==============================] - 28s 1ms/step - loss: 123299431507296256.0000 - val_loss: 255066751815909376.0000\n",
      "Epoch 57/1000\n",
      "19155/19155 [==============================] - 29s 1ms/step - loss: 123299225348866048.0000 - val_loss: 255066459758133248.0000\n",
      "Epoch 58/1000\n",
      "19155/19155 [==============================] - 29s 2ms/step - loss: 123299311248211968.0000 - val_loss: 255066236419833856.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/1000\n",
      "19155/19155 [==============================] - 30s 2ms/step - loss: 123298941881024512.0000 - val_loss: 255065927182188544.0000\n",
      "Epoch 60/1000\n",
      "19155/19155 [==============================] - 30s 2ms/step - loss: 123299002010566656.0000 - val_loss: 255065738203627520.0000\n",
      "Epoch 61/1000\n",
      "19155/19155 [==============================] - 30s 2ms/step - loss: 123297550311620608.0000 - val_loss: 255065463325720576.0000\n",
      "Epoch 62/1000\n",
      "19155/19155 [==============================] - 30s 2ms/step - loss: 123298177376845824.0000 - val_loss: 255065154088075264.0000\n",
      "Epoch 63/1000\n",
      "19155/19155 [==============================] - 29s 2ms/step - loss: 123298340585603072.0000 - val_loss: 255064793310822400.0000\n",
      "Epoch 64/1000\n",
      "19155/19155 [==============================] - 28s 1ms/step - loss: 123298048527826944.0000 - val_loss: 255064449713438720.0000\n",
      "Epoch 65/1000\n",
      "19155/19155 [==============================] - 28s 1ms/step - loss: 123297309793452032.0000 - val_loss: 255064174835531776.0000\n",
      "Epoch 66/1000\n",
      "19155/19155 [==============================] - 28s 1ms/step - loss: 123297301203517440.0000 - val_loss: 255063814058278912.0000\n",
      "Epoch 67/1000\n",
      "19155/19155 [==============================] - 28s 1ms/step - loss: 123297000555806720.0000 - val_loss: 255063573540110336.0000\n",
      "Epoch 68/1000\n",
      "19155/19155 [==============================] - 29s 2ms/step - loss: 123296682728226816.0000 - val_loss: 255063333021941760.0000\n",
      "Epoch 69/1000\n",
      "19155/19155 [==============================] - 28s 1ms/step - loss: 123296742857768960.0000 - val_loss: 255063040964165632.0000\n",
      "Epoch 70/1000\n",
      "19155/19155 [==============================] - 28s 1ms/step - loss: 123296502339600384.0000 - val_loss: 255062697366781952.0000\n",
      "Epoch 71/1000\n",
      "19155/19155 [==============================] - 28s 1ms/step - loss: 123296090022739968.0000 - val_loss: 255062456848613376.0000\n",
      "Epoch 72/1000\n",
      "19155/19155 [==============================] - 28s 1ms/step - loss: 123295325518561280.0000 - val_loss: 255062113251229696.0000\n",
      "Epoch 73/1000\n",
      "19155/19155 [==============================] - 28s 1ms/step - loss: 123296416440254464.0000 - val_loss: 255061855553191936.0000\n",
      "Epoch 74/1000\n",
      "19155/19155 [==============================] - 28s 1ms/step - loss: 123294724223139840.0000 - val_loss: 255061546315546624.0000\n",
      "Epoch 75/1000\n",
      "19155/19155 [==============================] - 28s 1ms/step - loss: 123295385648103424.0000 - val_loss: 255061185538293760.0000\n",
      "Epoch 76/1000\n",
      "19155/19155 [==============================] - 28s 1ms/step - loss: 123295325518561280.0000 - val_loss: 255060893480517632.0000\n",
      "Epoch 77/1000\n",
      "19155/19155 [==============================] - 28s 1ms/step - loss: 123294784352681984.0000 - val_loss: 255060549883133952.0000\n",
      "Epoch 78/1000\n",
      "19155/19155 [==============================] - 28s 1ms/step - loss: 123294612553990144.0000 - val_loss: 255060223465619456.0000\n",
      "Epoch 79/1000\n",
      "19155/19155 [==============================] - 28s 1ms/step - loss: 123294578194251776.0000 - val_loss: 255059845508497408.0000\n",
      "Epoch 80/1000\n",
      "19155/19155 [==============================] - 29s 2ms/step - loss: 123293873819615232.0000 - val_loss: 255059501911113728.0000\n",
      "Epoch 81/1000\n",
      "19155/19155 [==============================] - 28s 1ms/step - loss: 123293624711512064.0000 - val_loss: 255059123953991680.0000\n",
      "Epoch 82/1000\n",
      "19155/19155 [==============================] - 28s 1ms/step - loss: 123293521632296960.0000 - val_loss: 255058745996869632.0000\n",
      "Epoch 83/1000\n",
      "19155/19155 [==============================] - 28s 1ms/step - loss: 123292817257660416.0000 - val_loss: 255058368039747584.0000\n",
      "Epoch 84/1000\n",
      "19155/19155 [==============================] - 29s 2ms/step - loss: 123292387760930816.0000 - val_loss: 255058058802102272.0000\n",
      "Epoch 85/1000\n",
      "19155/19155 [==============================] - 28s 1ms/step - loss: 123291966854135808.0000 - val_loss: 255057646485241856.0000\n",
      "Epoch 86/1000\n",
      "19155/19155 [==============================] - 29s 1ms/step - loss: 123291760695705600.0000 - val_loss: 255057251348250624.0000\n",
      "Epoch 87/1000\n",
      "19155/19155 [==============================] - 31s 2ms/step - loss: 123291391328518144.0000 - val_loss: 255056821851521024.0000\n",
      "Epoch 88/1000\n",
      "19155/19155 [==============================] - 31s 2ms/step - loss: 123292284681715712.0000 - val_loss: 255056512613875712.0000\n",
      "Epoch 89/1000\n",
      "19155/19155 [==============================] - 29s 1ms/step - loss: 123292009803808768.0000 - val_loss: 255056083117146112.0000\n",
      "Epoch 90/1000\n",
      "19155/19155 [==============================] - 29s 1ms/step - loss: 123290343356497920.0000 - val_loss: 255055653620416512.0000\n",
      "Epoch 91/1000\n",
      "19155/19155 [==============================] - 30s 2ms/step - loss: 123290961831788544.0000 - val_loss: 255055172584079360.0000\n",
      "Epoch 92/1000\n",
      "19155/19155 [==============================] - 30s 2ms/step - loss: 123290171557806080.0000 - val_loss: 255054794626957312.0000\n",
      "Epoch 93/1000\n",
      "19155/19155 [==============================] - 28s 1ms/step - loss: 123290240277282816.0000 - val_loss: 255054382310096896.0000\n",
      "Epoch 94/1000\n",
      "19155/19155 [==============================] - 28s 1ms/step - loss: 123290497975320576.0000 - val_loss: 255053969993236480.0000\n",
      "Epoch 95/1000\n",
      "19155/19155 [==============================] - 28s 1ms/step - loss: 123289638981861376.0000 - val_loss: 255053523316637696.0000\n",
      "Epoch 96/1000\n",
      "19155/19155 [==============================] - 29s 1ms/step - loss: 123289389873758208.0000 - val_loss: 255053145359515648.0000\n",
      "Epoch 97/1000\n",
      "19155/19155 [==============================] - 28s 1ms/step - loss: 123288891657551872.0000 - val_loss: 255052715862786048.0000\n",
      "Epoch 98/1000\n",
      "19155/19155 [==============================] - 29s 2ms/step - loss: 123288745628663808.0000 - val_loss: 255052252006318080.0000\n",
      "Epoch 99/1000\n",
      "19155/19155 [==============================] - 29s 2ms/step - loss: 123287963944615936.0000 - val_loss: 255051874049196032.0000\n",
      "Epoch 100/1000\n",
      "19155/19155 [==============================] - 28s 1ms/step - loss: 123288264592326656.0000 - val_loss: 255051393012858880.0000\n",
      "Epoch 101/1000\n",
      "19155/19155 [==============================] - 29s 2ms/step - loss: 123287362649194496.0000 - val_loss: 255050980695998464.0000\n",
      "Epoch 102/1000\n",
      "19155/19155 [==============================] - 30s 2ms/step - loss: 123287422778736640.0000 - val_loss: 255050568379138048.0000\n",
      "Epoch 103/1000\n",
      "19155/19155 [==============================] - 30s 2ms/step - loss: 123286288907370496.0000 - val_loss: 255050190422016000.0000\n",
      "Epoch 104/1000\n",
      "19155/19155 [==============================] - 29s 2ms/step - loss: 123286907382661120.0000 - val_loss: 255049760925286400.0000\n",
      "Epoch 105/1000\n",
      "19155/19155 [==============================] - 30s 2ms/step - loss: 123286039799267328.0000 - val_loss: 255049297068818432.0000\n",
      "Epoch 106/1000\n",
      "19155/19155 [==============================] - 29s 2ms/step - loss: 123285953899921408.0000 - val_loss: 255048850392219648.0000\n",
      "Epoch 107/1000\n",
      "19155/19155 [==============================] - 30s 2ms/step - loss: 123285000417181696.0000 - val_loss: 255048403715620864.0000\n",
      "Epoch 108/1000\n",
      "19155/19155 [==============================] - 29s 2ms/step - loss: 123284897337966592.0000 - val_loss: 255047819600068608.0000\n",
      "Epoch 109/1000\n",
      "19155/19155 [==============================] - 31s 2ms/step - loss: 123285249525284864.0000 - val_loss: 255047424463077376.0000\n",
      "Epoch 110/1000\n",
      "19155/19155 [==============================] - 29s 2ms/step - loss: 123283789236404224.0000 - val_loss: 255046805987786752.0000\n",
      "Epoch 111/1000\n",
      "19155/19155 [==============================] - 31s 2ms/step - loss: 123284012574703616.0000 - val_loss: 255046342131318784.0000\n",
      "Epoch 112/1000\n",
      "19155/19155 [==============================] - 29s 2ms/step - loss: 123284321812348928.0000 - val_loss: 255045843915112448.0000\n",
      "Epoch 113/1000\n",
      "19155/19155 [==============================] - 28s 1ms/step - loss: 123283351149740032.0000 - val_loss: 255045345698906112.0000\n",
      "Epoch 114/1000\n",
      "19155/19155 [==============================] - 28s 1ms/step - loss: 123283162171179008.0000 - val_loss: 255044881842438144.0000\n",
      "Epoch 115/1000\n",
      "19155/19155 [==============================] - 28s 1ms/step - loss: 123282672544907264.0000 - val_loss: 255044332086624256.0000\n",
      "Epoch 116/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19155/19155 [==============================] - 30s 2ms/step - loss: 123281444184260608.0000 - val_loss: 255043851050287104.0000\n",
      "Epoch 117/1000\n",
      "19155/19155 [==============================] - 30s 2ms/step - loss: 123281547263475712.0000 - val_loss: 255043318474342400.0000\n",
      "Epoch 118/1000\n",
      "19155/19155 [==============================] - 31s 2ms/step - loss: 123281195076157440.0000 - val_loss: 255042906157481984.0000\n",
      "Epoch 119/1000\n",
      "19155/19155 [==============================] - 30s 2ms/step - loss: 123280945968054272.0000 - val_loss: 255042545380229120.0000\n",
      "Epoch 120/1000\n",
      "19155/19155 [==============================] - 30s 2ms/step - loss: 123280911608315904.0000 - val_loss: 255041944084807680.0000\n",
      "Epoch 121/1000\n",
      "19155/19155 [==============================] - 30s 2ms/step - loss: 123280044024922112.0000 - val_loss: 255041377149124608.0000\n",
      "Epoch 122/1000\n",
      "19155/19155 [==============================] - 33s 2ms/step - loss: 123280086974595072.0000 - val_loss: 255040827393310720.0000\n",
      "Epoch 123/1000\n",
      "19155/19155 [==============================] - 30s 2ms/step - loss: 123279210801266688.0000 - val_loss: 255040294817366016.0000\n",
      "Epoch 124/1000\n",
      "19155/19155 [==============================] - 30s 2ms/step - loss: 123278781304537088.0000 - val_loss: 255039882500505600.0000\n",
      "Epoch 125/1000\n",
      "19155/19155 [==============================] - 29s 2ms/step - loss: 123278798484406272.0000 - val_loss: 255039281205084160.0000\n",
      "Epoch 126/1000\n",
      "19155/19155 [==============================] - 29s 1ms/step - loss: 123278472066891776.0000 - val_loss: 255038800168747008.0000\n",
      "Epoch 127/1000\n",
      "19155/19155 [==============================] - 29s 2ms/step - loss: 123277501404282880.0000 - val_loss: 255038250412933120.0000\n",
      "Epoch 128/1000\n",
      "19155/19155 [==============================] - 29s 1ms/step - loss: 123277355375394816.0000 - val_loss: 255037649117511680.0000\n",
      "Epoch 129/1000\n",
      "19155/19155 [==============================] - 29s 2ms/step - loss: 123275723287822336.0000 - val_loss: 255037013462351872.0000\n",
      "Epoch 130/1000\n",
      "19155/19155 [==============================] - 29s 2ms/step - loss: 123276530741673984.0000 - val_loss: 255036463706537984.0000\n",
      "Epoch 131/1000\n",
      "19155/19155 [==============================] - 29s 2ms/step - loss: 123275766237495296.0000 - val_loss: 255035913950724096.0000\n",
      "Epoch 132/1000\n",
      "19155/19155 [==============================] - 30s 2ms/step - loss: 123275079042727936.0000 - val_loss: 255035226755956736.0000\n",
      "Epoch 133/1000\n",
      "19155/19155 [==============================] - 30s 2ms/step - loss: 123274623776194560.0000 - val_loss: 255034573920927744.0000\n",
      "Epoch 134/1000\n",
      "19155/19155 [==============================] - 29s 2ms/step - loss: 123274314538549248.0000 - val_loss: 255034006985244672.0000\n",
      "Epoch 135/1000\n",
      "19155/19155 [==============================] - 29s 2ms/step - loss: 123273962351230976.0000 - val_loss: 255033354150215680.0000\n",
      "Epoch 136/1000\n",
      "19155/19155 [==============================] - 30s 2ms/step - loss: 123273524264566784.0000 - val_loss: 255032787214532608.0000\n",
      "Epoch 137/1000\n",
      "19155/19155 [==============================] - 29s 2ms/step - loss: 123273154897379328.0000 - val_loss: 255032185919111168.0000\n",
      "Epoch 138/1000\n",
      "19155/19155 [==============================] - 29s 1ms/step - loss: 123272459112677376.0000 - val_loss: 255031618983428096.0000\n",
      "Epoch 139/1000\n",
      "19155/19155 [==============================] - 29s 1ms/step - loss: 123271814867582976.0000 - val_loss: 255031120767221760.0000\n",
      "Epoch 140/1000\n",
      "19155/19155 [==============================] - 27s 1ms/step - loss: 123272098335424512.0000 - val_loss: 255030467932192768.0000\n",
      "Epoch 141/1000\n",
      "19155/19155 [==============================] - 30s 2ms/step - loss: 123271273701703680.0000 - val_loss: 255029918176378880.0000\n",
      "Epoch 142/1000\n",
      "19155/19155 [==============================] - 30s 2ms/step - loss: 123270036751122432.0000 - val_loss: 255029299701088256.0000\n",
      "Epoch 143/1000\n",
      "19155/19155 [==============================] - 30s 2ms/step - loss: 123270345988767744.0000 - val_loss: 255028698405666816.0000\n",
      "Epoch 144/1000\n",
      "19155/19155 [==============================] - 31s 2ms/step - loss: 123269572894654464.0000 - val_loss: 255028079930376192.0000\n",
      "Epoch 145/1000\n",
      "19155/19155 [==============================] - 30s 2ms/step - loss: 123269718923542528.0000 - val_loss: 255027427095347200.0000\n",
      "Epoch 146/1000\n",
      "19155/19155 [==============================] - 30s 2ms/step - loss: 123268748260933632.0000 - val_loss: 255026739900579840.0000\n",
      "Epoch 147/1000\n",
      "19155/19155 [==============================] - 30s 2ms/step - loss: 123268396073615360.0000 - val_loss: 255025983986335744.0000\n",
      "Epoch 148/1000\n",
      "19155/19155 [==============================] - 31s 2ms/step - loss: 123267786188259328.0000 - val_loss: 255025382690914304.0000\n",
      "Epoch 149/1000\n",
      "19155/19155 [==============================] - 32s 2ms/step - loss: 123266695266566144.0000 - val_loss: 255024781395492864.0000\n",
      "Epoch 150/1000\n",
      "19155/19155 [==============================] - 32s 2ms/step - loss: 123267038863949824.0000 - val_loss: 255024214459809792.0000\n",
      "Epoch 151/1000\n",
      "19155/19155 [==============================] - 30s 2ms/step - loss: 123266351669182464.0000 - val_loss: 255023561624780800.0000\n",
      "Epoch 152/1000\n",
      "19155/19155 [==============================] - 30s 2ms/step - loss: 123265922172452864.0000 - val_loss: 255023011868966912.0000\n",
      "Epoch 153/1000\n",
      "19155/19155 [==============================] - 30s 2ms/step - loss: 123265372416638976.0000 - val_loss: 255022307494330368.0000\n",
      "Epoch 154/1000\n",
      "19155/19155 [==============================] - 31s 2ms/step - loss: 123264616502394880.0000 - val_loss: 255021534400217088.0000\n",
      "Epoch 155/1000\n",
      "19155/19155 [==============================] - 31s 2ms/step - loss: 123264539192983552.0000 - val_loss: 255020744126234624.0000\n",
      "Epoch 156/1000\n",
      "19155/19155 [==============================] - 33s 2ms/step - loss: 123264015206973440.0000 - val_loss: 255020194370420736.0000\n",
      "Epoch 157/1000\n",
      "19155/19155 [==============================] - 33s 2ms/step - loss: 123263302242402304.0000 - val_loss: 255019438456176640.0000\n",
      "Epoch 158/1000\n",
      "19155/19155 [==============================] - 30s 2ms/step - loss: 123262743896653824.0000 - val_loss: 255018613822455808.0000\n",
      "Epoch 159/1000\n",
      "19155/19155 [==============================] - 31s 2ms/step - loss: 123262211320709120.0000 - val_loss: 255017926627688448.0000\n",
      "Epoch 160/1000\n",
      "19155/19155 [==============================] - 31s 2ms/step - loss: 123261472586334208.0000 - val_loss: 255017136353705984.0000\n",
      "Epoch 161/1000\n",
      "19155/19155 [==============================] - 31s 2ms/step - loss: 123261515536007168.0000 - val_loss: 255016466338807808.0000\n",
      "Epoch 162/1000\n",
      "19155/19155 [==============================] - 30s 2ms/step - loss: 123260269995491328.0000 - val_loss: 255015727604432896.0000\n",
      "Epoch 163/1000\n",
      "19155/19155 [==============================] - 31s 2ms/step - loss: 123259754599415808.0000 - val_loss: 255014971690188800.0000\n",
      "Epoch 164/1000\n",
      "19155/19155 [==============================] - 30s 2ms/step - loss: 123260373074706432.0000 - val_loss: 255014284495421440.0000\n",
      "Epoch 165/1000\n",
      "19155/19155 [==============================] - 31s 2ms/step - loss: 123259436771835904.0000 - val_loss: 255013562940915712.0000\n",
      "Epoch 166/1000\n",
      "19155/19155 [==============================] - 31s 2ms/step - loss: 123258337260208128.0000 - val_loss: 255012721127325696.0000\n",
      "Epoch 167/1000\n",
      "19126/19155 [============================>.] - ETA: 0s - loss: 123410894498562048.0000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6236/792652251.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhistory\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1418\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1419\u001b[0m                 steps_per_execution=self._steps_per_execution)\n\u001b[1;32m-> 1420\u001b[1;33m           val_logs = self.evaluate(\n\u001b[0m\u001b[0;32m   1421\u001b[0m               \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1422\u001b[0m               \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m   1714\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'test'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep_num\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_r\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1715\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1716\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1717\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1718\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 915\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    952\u001b[0m       \u001b[1;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    953\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 954\u001b[1;33m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    955\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mALLOW_DYNAMIC_VARIABLE_CREATION\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    956\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2954\u001b[0m       (graph_function,\n\u001b[0;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2956\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2957\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2958\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1851\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1852\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1853\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1854\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 499\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    500\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history= model.fit(X_train, y_train, verbose=1, epochs = 1000, validation_data= (X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "aa034bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_test= model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "2644a821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "eb18d102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 886.7754  ]\n",
      " [1224.096   ]\n",
      " [2321.678   ]\n",
      " ...\n",
      " [3447.761   ]\n",
      " [  69.689026]\n",
      " [1858.0857  ]]\n"
     ]
    }
   ],
   "source": [
    "print(prediction_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "4c3e1652",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 311. GiB for an array with shape (204313, 204313) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6236/3135983258.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'mean sq error between y_test and predicted:'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprediction_test\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 311. GiB for an array with shape (204313, 204313) and data type float64"
     ]
    }
   ],
   "source": [
    "print('mean sq error between y_test and predicted:',np.mean(prediction_test-y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "21941d32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss= history.history['loss']\n",
    "val_loss=history.history['val_loss']\n",
    "epochs = range(1,len(loss)+1)\n",
    "plt.plot(epochs,loss, 'y', label= 'Training loss')\n",
    "plt.plot(epochs, val_loss,'r', label= 'validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b239c5da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
